# Procesamiento de Datos con Hadoop

Este proyecto implementa varios ejercicios para procesar datos en Hadoop utilizando MapReduce. Los datos analizados provienen de un archivo llamado `purchases.txt`, almacenado en HDFS.

## Tabla de Contenidos
- [Descripción](#descripción)
- [Requisitos](#requisitos)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Ejercicios](#ejercicios)
  - [Exercicio 1.1](#exercicio-11)
  - [Exercicio 1.2](#exercicio-12)
  - [Exercicio 1.3](#exercicio-13)
  - [Exercicio 1.4](#exercicio-14)
  - [Exercicio 1.5](#exercicio-15)
- [Autores](#autores)

## Descripción

El objetivo principal del proyecto es demostrar cómo procesar grandes volúmenes de datos utilizando algoritmos distribuidos con Hadoop y MapReduce. Cada ejercicio aborda un problema específico relacionado con la limpieza de datos y el cálculo de estadísticas como totales, máximos y agrupaciones.

## Requisitos

1. **Hadoop** instalado y configurado.
2. Archivo de datos `purchases.txt` cargado en el sistema de archivos HDFS.
3. Python para implementar los mappers y reducers (en función de tus preferencias).
4. Conexión a un clúster Hadoop funcional.

## Estructura del Proyecto
1. mapper.py Para el Ejercicio1_1
   
